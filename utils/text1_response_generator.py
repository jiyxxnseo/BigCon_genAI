import re
import faiss
import torch
from utils.faiss_utils import load_faiss_index
from utils.config import model, config

# Multi-turn ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
multi_turn_context = []

# ì·¨ì†Œì„  ë° ë³¼ë“œ ë¬¸ë²• ì œê±° í•¨ìˆ˜
def clean_text(text):
    # ~~ì·¨ì†Œì„ ~~ ë¬¸ë²• ì œê±°
    text = re.sub(r"~~(.*?)~~", r"\1", text)
    # ~ì·¨ì†Œì„ ~ ë¬¸ë²• ì œê±°
    text = re.sub(r"~(.*?)~", r"\1", text)
    # **"í…ìŠ¤íŠ¸"**ë¥¼ <b>"í…ìŠ¤íŠ¸"</b>ë¡œ ë³€í™˜
    text = re.sub(r'\*\*"(.*?)"\*\*', r'<b>"\1"</b>', text)
    # **í…ìŠ¤íŠ¸**ë¥¼ <b>í…ìŠ¤íŠ¸</b>ë¡œ ë³€í™˜
    text = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', text)
    return text

# Main function to generate response using FAISS and Gemini
def generate_response_with_faiss(question, df, embeddings, model, embed_text, k=3):
    index_path = config['faiss']['faiss_index']
    print(index_path)

    # Load FAISS index
    # index = load_faiss_index(index_path)

    # í•„í„°ë§ëœ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ
    filtered_indices = df.index.tolist()

    # ì¶”ì¶œëœ ì¸ë±ìŠ¤ë¥¼ í†µí•´ ì„ë² ë”© ë°ì´í„°ì—ì„œ í•„í„°ë§ëœ ë°ì´í„°ë§Œ ì¶”ì¶œ
    filtered_embeddings = embeddings[filtered_indices]

    # Query embedding
    query_embedding = embed_text(question).reshape(1, -1)

    # FAISS ì¸ë±ìŠ¤ ë¹Œë“œ
    dimension = filtered_embeddings.shape[1]
    faiss_index = faiss.IndexFlatL2(dimension)
    faiss_index.add(filtered_embeddings)

    # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (3ë°°ìˆ˜)
    distances, indices = faiss_index.search(query_embedding, k * 3)

    # FAISSë¡œ ê²€ìƒ‰ëœ ìƒìœ„ kê°œì˜ ë°ì´í„°í”„ë ˆì„ ì¶”ì¶œ
    filtered_df = df.iloc[indices[0, :]].copy().reset_index(drop=True)

    reference_info = ""
    for idx, row in filtered_df.iterrows():
        reference_info += f"{row['text']}\n"

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    conversation_history = "\n".join(multi_turn_context)
    prompt = f"{conversation_history}\nì§ˆë¬¸: {question}\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µì€ ì¹œì ˆí•˜ê²Œ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì²˜ëŸ¼:"

    print(model.count_tokens(prompt))
    try:
        response = model.generate_content(prompt)
        # Print the response text in the terminal
        if response._result and response._result.candidates:
            generated_text = response._result.candidates[0].content.parts[0].text
            # ì·¨ì†Œì„  ë° ë³¼ë“œ ë¬¸ë²• ì œê±°
            generated_text = clean_text(generated_text)

        else:
            generated_text = "ì£„ì†¡í•´ìš”. ì¶”ì²œì— í•„ìš”í•œ ì •ë³´ê°€ ì¡°ê¸ˆ ë¶€ì¡±í•œ ê²ƒ ê°™ì•„ìš”ğŸ¥² êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ê·¸ì— ë”± ë§ëŠ” ë©‹ì§„ ê³³ì„ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”!ğŸ¥°"
            print("No valid response generated.")

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì§ˆë¬¸ê³¼ ì‘ë‹µ ì¶”ê°€
        multi_turn_context.append(f"ì§ˆë¬¸: {question}")
        multi_turn_context.append(f"ì‘ë‹µ: {generated_text}")
        
    except Exception as e:
        print(f"Error occurred: {e}")
        generated_text = "ì£„ì†¡í•´ìš”. ì¶”ì²œì— í•„ìš”í•œ ì •ë³´ê°€ ì¡°ê¸ˆ ë¶€ì¡±í•œ ê²ƒ ê°™ì•„ìš”ğŸ¥² êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ê·¸ì— ë”± ë§ëŠ” ë©‹ì§„ ê³³ì„ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”!ğŸ¥°"

    return generated_text

# Function to generate response based on SQL query results
def generate_gemini_response_from_results(sql_results, question):
    if sql_results.empty:
        return "ì£„ì†¡í•´ìš”. ì¶”ì²œì— í•„ìš”í•œ ì •ë³´ê°€ ì¡°ê¸ˆ ë¶€ì¡±í•œ ê²ƒ ê°™ì•„ìš”ğŸ¥² êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ê·¸ì— ë”± ë§ëŠ” ë©‹ì§„ ê³³ì„ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”!ğŸ¥°"

    best_match = sql_results.iloc[:3]
    print(best_match)
    reference_info = ""
    for idx, row in best_match.iterrows():
        reference_info += f"{row['text']}\n"

    conversation_history = "\n".join(multi_turn_context)
    prompt = f"{conversation_history}\nì§ˆë¬¸: {question}\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µì€ ìµœëŒ€í•œ ì¹œì ˆí•˜ê³  ì¹œê·¼í•˜ê²Œ ì‹ë‹¹ ì¶”ì²œí•´ì£¼ëŠ” ì±—ë´‡ì²˜ëŸ¼:"
    print("input_tokens: ", model.count_tokens(prompt))
    print("sql_ì°¸ê³ ìë£Œ tokens: ", model.count_tokens(reference_info))

    try:
        response = model.generate_content(prompt)
        if hasattr(response, 'candidates') and response.candidates:
            for candidate in response.candidates:
                for part in candidate.content.parts:
                    if hasattr(part, 'text'):
                        response_text = part.text
                        # ì·¨ì†Œì„  ì œê±°
                        response_text = clean_text(response_text)
                        
        else:
            response_text = "ì£„ì†¡í•´ìš”. ì¶”ì²œì— í•„ìš”í•œ ì •ë³´ê°€ ì¡°ê¸ˆ ë¶€ì¡±í•œ ê²ƒ ê°™ì•„ìš”ğŸ¥² êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ê·¸ì— ë”± ë§ëŠ” ë©‹ì§„ ê³³ì„ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”!ğŸ¥°"
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì§ˆë¬¸ê³¼ ì‘ë‹µ ì¶”ê°€
        multi_turn_context.append(f"ì§ˆë¬¸: {question}")
        multi_turn_context.append(f"ì‘ë‹µ: {response_text}")
    
    except Exception as e:
        print(f"Error occurred: {e}")
        response_text = "ì£„ì†¡í•´ìš”. ì¶”ì²œì— í•„ìš”í•œ ì •ë³´ê°€ ì¡°ê¸ˆ ë¶€ì¡±í•œ ê²ƒ ê°™ì•„ìš”ğŸ¥² êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ê·¸ì— ë”± ë§ëŠ” ë©‹ì§„ ê³³ì„ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”!ğŸ¥°"

    return response_text
